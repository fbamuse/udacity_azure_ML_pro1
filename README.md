#Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about ... we seek to predict ..."**  
This data set is the result of a direct marketing campaign on bank time deposits.
It contains about 20 information such as age, work, family structure, and information on whether or not you have purchased a time deposit. This project predicts whether or not to contract a fixed deposit from the attribute information of the customer.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**  
Logistic regression Acurracy had a 0.913 calculation time of 11 minutes and AutoML had a 0.917 calculation time of 42 minutes. In terms of score, AutoML has an advantage. However, the calculation time is long and the advantage is lost. However, it has a highly readable explanatory model attached to it, which meets the essential requirements for dealing with sensitive issues. Reducing calculation time is a problem that can be avoided by cluster configuration.


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**  
Dandam sampling was performed with the two parameters of regularization strength and maximum number of iterations. This parameter tuning is tuning to bring out generalization performance.

**What are the benefits of the parameter sampler you chose?**  
We chose random sampling, which makes it easier to allocate the best combination with fewer samplings than grid search.

**What are the benefits of the early stopping policy you chose?**  
If the specified margin causes the target performance metric to fall below the best run ever, the bandit policy is used to stop the run.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML**  
You can select the best model by evaluating the combination of pretreatment method and classifier, which is difficult to do manually. The automation of pre-processing such as missing value handling and normalization is wonderful. Model selection seems to work to gradually narrow down the best model from the situation as a result of previous learning.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**  
In the results of hyper-tuning of logistics regression and prediction by AutoML, AutoML gave better prediction results. As a result of AutoML exploring various classifiers, the boosting method yielded the best results. It is possible that robust boosting was superior due to unbalanced data.
In addition, because it is necessary to shorten the calculation time, the number of clusters can be increased at one time and learning can be completed early, so the use of AutoML, which involves a huge amount of calculation in cloud computing, is advantageous.
## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**  
First of all, you can expect even better results with AutoML by correcting the unbalanced data alerted by the inspection function per AutoML.
## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.** 
**Image of cluster marked for deletion**   